#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "wb") as f:
                        f.write(b"\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "wb") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('color_detector.py', b'import cv2\r\nimport numpy as np\r\nfrom src.color_defs import *\r\nfrom src.block import Block\r\nfrom typing import List, TypedDict\r\nfrom src.detector import *\r\nfrom src.type_defs import *\r\nfrom enum import Enum\r\n\r\n\r\nclass ColorDetector(Detector):\r\n    """\r\n    Detects color blocks by:\r\n      1) Preprocessing (brightness, blur)\r\n      2) Creating color masks\r\n      3) Finding contours\r\n      4) Computing mean & std(H, S, V) inside each contour\r\n    """\r\n    class DebugType(Enum):\r\n        COLOR_MASK = "color_mask"\r\n        COMBINED_COLOR_MASK = "combined_color_mask"\r\n\r\n    def __init__(self, detecting_colors: List[Color], preproc_cfg: PreprocCfg = PreprocCfg(), debug_option : List[DebugType] | bool = []):\r\n        # Basic image processing parameters\r\n        super().__init__(detecting_colors, preproc_cfg, debug_option, DebugType)\r\n        self.min_contour_area = 1000\r\n        # Thresholds for std(H, S, V)\r\n        self.std_threshold_hsv = (3, 50, 50)\r\n\r\n    def process_frame(self, frame: np.ndarray) -> List[Block]:\r\n        """Main entry: preprocess and detect blocks, while saving debug images."""\r\n        # 1) Preprocessing\r\n        preprocessed = self._preprocess(frame)\r\n\r\n        # 2) Convert to HSV\r\n        hsv = cv2.cvtColor(preprocessed, cv2.COLOR_BGR2HSV)\r\n\r\n        # 3) Detect blocks\r\n        blocks = self._detect_blocks(hsv)\r\n\r\n        return blocks\r\n\r\n    def _detect_blocks(self, hsv: np.ndarray) -> List[Block]:\r\n        blocks = []\r\n        combined_mask = np.zeros(hsv.shape[:2], dtype=np.uint8)\r\n\r\n        for color_def in self.detecting_colors:\r\n            mask = self.create_color_mask(hsv, color_def)\r\n            if DebugType.COLOR_MASK in self.debug_option:\r\n                mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\r\n                mask_bgr[mask == 255] = color_def.bgr\r\n                self.debug_images[f\'mask_{color_def.name}\'] = mask_bgr\r\n            combined_mask = cv2.bitwise_or(combined_mask, mask)\r\n\r\n            contours = self._find_contours(mask)\r\n            color_blocks = self._process_contours(contours, color_def, hsv)\r\n            blocks.extend(color_blocks)\r\n\r\n        if DebugType.COMBINED_COLOR_MASK in self.debug_option:\r\n            self.debug_images[\'combined_mask\'] = self._merge_debug_imgs(\'mask\')\r\n\r\n        return blocks\r\n\r\n    def _find_contours(self, mask: np.ndarray) -> List[np.ndarray]:\r\n        """Find external contours with area > min_contour_area."""\r\n        contours, _ = cv2.findContours(\r\n            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n        return [c for c in contours if cv2.contourArea(c) > self.min_contour_area]\r\n\r\n    def _process_contours(self,\r\n                          contours: List[np.ndarray],\r\n                          color_def: Color,\r\n                          hsv: np.ndarray) -> List[Block]:\r\n        """Compute mean & std(H, S, V) inside each contour and filter by thresholds."""\r\n        blocks = []\r\n        for cnt in contours:\r\n            rect = cv2.minAreaRect(cnt)\r\n            (cx, cy), (w, h), angle = rect\r\n\r\n            # Normalize orientation\r\n            if w < h:\r\n                w, h = h, w\r\n                angle += 90\r\n\r\n            # Get bounding rect (for ROI)\r\n            x_min, y_min, w_int, h_int = cv2.boundingRect(cnt)\r\n            if w_int == 0 or h_int == 0:\r\n                continue\r\n\r\n            # Create local mask (contour inside bounding box)\r\n            contour_mask = np.zeros((h_int, w_int), dtype=np.uint8)\r\n            shifted_cnt = cnt - [x_min, y_min]\r\n            cv2.drawContours(contour_mask, [shifted_cnt], 0, (255,), -1)\r\n\r\n            # Extract HSV ROI\r\n            hsv_roi = hsv[y_min:y_min + h_int, x_min:x_min + w_int]\r\n            hsv_masked = cv2.bitwise_and(hsv_roi, hsv_roi, mask=contour_mask)\r\n\r\n            # Split channels and extract valid pixels\r\n            h_ch, s_ch, v_ch = cv2.split(hsv_masked)\r\n            h_valid = h_ch[contour_mask == 255].astype(np.float32)\r\n            s_valid = s_ch[contour_mask == 255].astype(np.float32)\r\n            v_valid = v_ch[contour_mask == 255].astype(np.float32)\r\n\r\n            if len(h_valid) == 0:\r\n                continue\r\n\r\n            # Compute mean & std for H, S, V\r\n            mean_h = float(np.mean(h_valid))\r\n            mean_s = float(np.mean(s_valid))\r\n            mean_v = float(np.mean(v_valid))\r\n\r\n            std_h = compute_hue_std_flip(h_valid, flip_threshold=90.0)\r\n            std_s = float(np.std(s_valid))\r\n            std_v = float(np.std(v_valid))\r\n\r\n            # Create a new Block\r\n            if std_h <= self.std_threshold_hsv[0] and \\\r\n               std_s <= self.std_threshold_hsv[1] and \\\r\n               std_v <= self.std_threshold_hsv[2]:\r\n\r\n                block = Block(\r\n                    center=(cx, cy),\r\n                    size=(w, h),\r\n                    angle=angle,\r\n                    color=color_def,\r\n                    color_std=(std_h, std_s, std_v),\r\n                    mean_hsv=(mean_h, mean_s, mean_v),\r\n                    # store the original contour (absolute coordinates)\r\n                    contour=cnt\r\n                )\r\n                blocks.append(block)\r\n        return blocks\r\n')
    __stickytape_write_module('src/color_defs.py', b'import numpy as np\r\nfrom dataclasses import dataclass, field\r\nfrom typing import List, Tuple\r\nfrom src.type_defs import hsv_t, bgr_t\r\n\r\n@dataclass\r\nclass Color:\r\n    """Stores color name, HSV ranges, and BGR values for drawing."""\r\n    name: str\r\n    hsv_ranges: List[Tuple[hsv_t, hsv_t]]\r\n    bgr: bgr_t\r\n\r\n\r\n# ---------- Color Block Detector ----------\r\n\r\ndef compute_hue_std_flip(h_array: np.ndarray, flip_threshold: float = 90.0) -> float:\r\n    # Ensure float\r\n    h_float = h_array.astype(np.float32)\r\n\r\n    # 1) Direct std\r\n    std1 = np.std(h_float)\r\n\r\n    # 2) Flip\r\n    shifted = h_float.copy()\r\n    mask = (shifted < flip_threshold)\r\n    shifted[mask] += 180.0\r\n    std2 = np.std(shifted)\r\n\r\n    return float(min(std1, std2))\r\n\r\n\r\nRED_R9000P = Color(\r\n    name="RED_R9000P",\r\n    hsv_ranges=[\r\n        ((0, 70, 50), (3, 160, 225)),\r\n        ((165, 70, 50), (180, 160, 225)),\r\n    ],\r\n    bgr=(0, 0, 255)\r\n)\r\n\r\nBLUE_R9000P = Color(\r\n    name="BLUE_R9000P",\r\n    hsv_ranges=[\r\n        ((110, 80, 70), (125, 180, 230)),\r\n    ],\r\n    bgr=(255, 0, 0)\r\n)\r\n\r\nYELLOW_R9000P = Color(\r\n    name="YELLOW_R9000P",\r\n    hsv_ranges=[\r\n        ((17, 60, 140), (32, 125, 255)),\r\n    ],\r\n    bgr=(0, 255, 255)\r\n)\r\n\r\nCOLOR_DEF_R9000P = [RED_R9000P, BLUE_R9000P, YELLOW_R9000P]\r\n\r\nRED_LL = Color(\r\n    name="RED_LL",\r\n    hsv_ranges=[\r\n        ((0, 190, 90), (5, 255, 250)),\r\n        ((160, 190, 90), (180, 255, 250)),\r\n    ],\r\n    bgr=(0, 0, 255)\r\n)\r\n\r\nYELLOW_LL = Color(\r\n    name="YELLOW_LL",\r\n    hsv_ranges=[\r\n        ((15, 130, 160), (35, 255, 250)),\r\n    ],\r\n    bgr=(0, 255, 255)\r\n)\r\n\r\nBLUE_LL = Color(\r\n    name="BLUE_LL",\r\n    hsv_ranges=[\r\n        ((100, 220, 70), (125, 255, 230)),\r\n    ],\r\n    bgr=(255, 0, 0)\r\n)\r\n\r\nCOLOR_DEF_LL = [RED_LL, YELLOW_LL, BLUE_LL]')
    __stickytape_write_module('src/type_defs.py', b"from typing import Tuple\r\nimport numpy as np \r\nimport numpy.typing as npt\r\nfrom typing import TypeVar, Annotated, Literal, Dict\r\n\r\nhsv_t = Tuple[int, int, int]\r\nbgr_t = Tuple[int, int, int]\r\n\r\n\r\nDtype = TypeVar('Dtype', bound=np.generic)\r\n\r\n\r\narray_NxNx3_t = Annotated[npt.NDArray[Dtype], Literal['N', 'N', 3]]\r\narray_NxNx1_t = Annotated[npt.NDArray[Dtype], Literal['N', 'N', 1]]\r\n\r\nimg_t = array_NxNx3_t\r\n\r\nimg_hsv_t = array_NxNx3_t\r\n\r\nimg_bgr_t = array_NxNx3_t\r\n\r\nimg_gray_t = array_NxNx1_t\r\n\r\nVizResults = Dict[str, img_t | img_gray_t]")
    __stickytape_write_module('src/block.py', b'import numpy as np\r\nfrom dataclasses import dataclass, field\r\nfrom src.color_defs import Color\r\nfrom typing import List, Tuple\r\n\r\n@dataclass\r\nclass Block:\r\n    """Represents a detected color block with position, size, angle, color info, and HSV stats."""\r\n    center: Tuple[float, float]\r\n    size: Tuple[float, float]\r\n    angle: float\r\n    color: Color\r\n    color_std: Tuple[float, float, float] = (0.0, 0.0, 0.0)\r\n    mean_hsv: Tuple[float, float, float] = (0.0, 0.0, 0.0)\r\n    # store the absolute contour for visualization\r\n    contour: np.ndarray = field(default_factory=lambda: np.array([]))\r\n')
    __stickytape_write_module('src/detector.py', b'from abc import ABC, abstractmethod\r\nfrom src.block import Block\r\nfrom typing import List, Sequence, Type\r\nimport numpy as np\r\nfrom dataclasses import dataclass\r\nfrom src.type_defs import *\r\nfrom src.color_defs import *\r\nfrom src.preprocessor import *\r\n\r\nEnumSub = TypeVar(\'EnumSub\', bound=Enum)\r\n\r\nclass Detector(ABC):\r\n\r\n\r\n    def __init__(self, detecting_colors: List[Color], \r\n                preproc_cfg: PreprocCfg, \r\n                debug_option: Sequence[EnumSub] | bool, \r\n                debug_type: Type[EnumSub]):\r\n        self.preproc_cfg = preproc_cfg\r\n        self.preproc = Preproc(self.preproc_cfg)\r\n        self.detecting_colors = detecting_colors\r\n        self.debug_images: VizResults = {}\r\n        if isinstance(debug_option, bool):\r\n            debug_option = list(debug_type) if debug_option else []\r\n        self.debug_option = debug_option\r\n\r\n    @property\r\n    @abstractmethod\r\n    def DebugType(self) -> Type[EnumSub]:\r\n        pass\r\n\r\n    @abstractmethod\r\n    def process_frame(self, frame: img_bgr_t) -> List[Block]:\r\n        pass\r\n\r\n    def _preprocess(self, frame: img_bgr_t) -> img_bgr_t:\r\n        ret = self.preproc.process(frame)\r\n        self.debug_images.update(self.preproc.debug_images)\r\n        return ret\r\n\r\n    @staticmethod\r\n    def create_color_mask(frame_hsv: img_hsv_t, colors: List[Color] | Color) -> img_gray_t:\r\n        mask = np.zeros(frame_hsv.shape[:2], dtype=np.uint8)\r\n        if isinstance(colors, Color):\r\n            colors = [colors]\r\n        for color in colors:\r\n            for (lower, upper) in color.hsv_ranges:\r\n                temp_mask = cv2.inRange(\r\n                    frame_hsv, np.array(lower), np.array(upper))\r\n                mask = cv2.bitwise_or(mask, temp_mask)\r\n        return mask\r\n\r\n    def _merge_debug_imgs(self, prefix: str) -> img_bgr_t:\r\n        """\r\n        Merge debug images with the given prefix according to order of `self.detecting_colors`\r\n        Later images have higher priority\r\n        """\r\n        ret = None\r\n        for color in self.detecting_colors:\r\n            img_name = f"{prefix}_{color.name}"\r\n            if img_name not in self.debug_images:\r\n                continue\r\n            img = self.debug_images[img_name]\r\n            if ret is None:\r\n                ret = np.zeros_like(img)\r\n            # Create a mask where the current image is not black (i.e., has color)\r\n            mask = np.any(img > 0, axis=-1)\r\n            ret[mask > 0] = img[mask > 0]\r\n        assert ret is not None, f"No debug images found with prefix {prefix}"\r\n        return ret\r\n')
    __stickytape_write_module('src/preprocessor.py', b'from typing import Tuple, List, Optional, Set, TypeVar, Annotated, Literal, TypedDict\r\nimport numpy as np\r\nimport numpy.typing as npt\r\nfrom dataclasses import dataclass, field\r\nimport cv2\r\nfrom enum import Enum\r\nfrom src.type_defs import *\r\n\r\n\r\nclass PreprocType(Enum):\r\n    """Enumeration of available preprocessing steps."""\r\n    AUTO_WB = "auto_wb"                # Auto White Balance\r\n    BRIGHTNESS = "brightness"          # Brightness Adjustment\r\n    HIST_EQUALIZE = "hist_equalize"    # Histogram Equalization\r\n    CLAHE = "clahe"                    # CLAHE\r\n    GAUSSIAN = "gaussian"              # Gaussian Blur\r\n    MEDIAN = "median"                  # Median Blur\r\n    BILATERAL = "bilateral"            # Bilateral Filter\r\n    GUIDED = "guided"                  # Guided Filter\r\n    MORPH_OPEN = "morph_open"          # Morphological Opening\r\n    MORPH_CLOSE = "morph_close"        # Morphological Closing\r\n# TODO: sharpening\r\n\r\nDEF_STEPS = [\r\n    PreprocType.MORPH_OPEN, \r\n    # PreprocType.MORPH_CLOSE,\r\n    PreprocType.BILATERAL,\r\n]\r\n\r\n# ------------------- Preprocess Configuration -------------------\r\n\r\n@dataclass\r\nclass PreprocCfg:\r\n    """Configuration for image preprocessing steps."""\r\n    preprocess_steps: List[PreprocType] = field(default_factory=lambda: DEF_STEPS)\r\n    # Steps for which debug output is desired\r\n    debug_steps: Set[PreprocType] | bool = field(default_factory=set)\r\n\r\n    brightness: int = 0\r\n\r\n    CLAHE_clip_limit: int = 2\r\n    CLAHE_grid_size: int = 8\r\n\r\n    gaussian_kernel_size: int = 5\r\n    gaussian_sigma: int = 0\r\n\r\n    median_kernel_size: int = 5\r\n\r\n    bilateral_d: int = 11\r\n    bilateral_sigma_color: int = 200\r\n    bilateral_sigma_space: int = 20\r\n\r\n    guided_radius: int = 5\r\n    guided_eps: float = 0.1\r\n\r\n    morph_open_kernel_size: int = 5\r\n    morph_open_iter: int = 1\r\n\r\n    morph_close_kernel_size: int = 5\r\n    morph_close_iter: int = 1\r\n\r\n# ------------------- Preprocessor Class -------------------\r\n\r\n\r\nclass Preproc:\r\n    """Applies a sequence of preprocessing steps to an image based on the given configuration.\r\n    Also outputs debug images for steps specified in cfg.debug_steps.\r\n    """\r\n\r\n    def __init__(self, cfg: PreprocCfg):\r\n        self.cfg = cfg\r\n        self.debug_images: VizResults = {}  # Store intermediate debug results\r\n        # Check if any debug step is specified but not present in the preprocess_steps\r\n\r\n        if isinstance(self.cfg.debug_steps, bool):\r\n            if self.cfg.debug_steps:\r\n                self.cfg.debug_steps = {step for step in self.cfg.preprocess_steps}\r\n            else: self.cfg.debug_steps = set()\r\n            # if true then add all steps to debug_steps\r\n            return\r\n    \r\n        if len(self.cfg.debug_steps) > 0:\r\n            missing_debug = {\r\n                step for step in self.cfg.debug_steps if not self.cfg.preprocess_steps or step not in self.cfg.preprocess_steps}\r\n            if missing_debug:\r\n                for step in missing_debug:\r\n                    print(\r\n                        f"Warning: Debug step \'{step.value}\' is specified but not included in preprocess_steps.")\r\n\r\n    def process(self, image: img_t) -> img_t:\r\n        """Processes the image based on the configured preprocessing steps.\r\n        Stores intermediate debug outputs for steps specified in debug_steps.\r\n        """\r\n        if not self.cfg.preprocess_steps:\r\n            return image\r\n\r\n        for step in self.cfg.preprocess_steps:\r\n            image = self._apply_step(image, step)\r\n            # If the current step is in the debug_steps, store a copy of the intermediate result.\r\n            if isinstance(self.cfg.debug_steps, set) and step in self.cfg.debug_steps:\r\n                self.debug_images[step.value] = image.copy()\r\n        return image\r\n\r\n    def _apply_step(self, image: img_t, step: PreprocType) -> img_t:\r\n        """Applies a specific preprocessing step to the image."""\r\n        processors = {\r\n            PreprocType.AUTO_WB: self._auto_white_balance,\r\n            PreprocType.BRIGHTNESS: self._adjust_brightness,\r\n            PreprocType.HIST_EQUALIZE: self._hist_equalize,\r\n            PreprocType.CLAHE: self._clahe,\r\n            PreprocType.GAUSSIAN: self._gaussian,\r\n            PreprocType.MEDIAN: self._median,\r\n            PreprocType.BILATERAL: self._bilateral,\r\n            PreprocType.GUIDED: self._guided_filter,\r\n            PreprocType.MORPH_OPEN: self._morph_open,\r\n            PreprocType.MORPH_CLOSE: self._morph_close,\r\n        }\r\n        return processors[step](image)\r\n\r\n    def _auto_white_balance(self, image: img_bgr_t) -> img_bgr_t:\r\n        """Applies automatic white balance using OpenCV\'s built-in method if available."""\r\n        if hasattr(cv2, "xphoto") and hasattr(cv2.xphoto, "createSimpleWB"):\r\n            wb = cv2.xphoto.createSimpleWB()\r\n            return wb.balanceWhite(image)\r\n        else:\r\n            # Fallback method: Normalize LAB color space (less accurate)\r\n            result = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\r\n            avg_a = np.mean(result[:, :, 1])\r\n            avg_b = np.mean(result[:, :, 2])\r\n            result[:, :, 1] = result[:, :, 1] - \\\r\n                ((avg_a - 128) * (result[:, :, 0] / 255.0))\r\n            result[:, :, 2] = result[:, :, 2] - \\\r\n                ((avg_b - 128) * (result[:, :, 0] / 255.0))\r\n            return cv2.cvtColor(result, cv2.COLOR_LAB2BGR)\r\n\r\n    def _adjust_brightness(self, image: img_t) -> img_t:\r\n        """Adjusts the brightness of the image."""\r\n        return cv2.convertScaleAbs(image, alpha=1, beta=self.cfg.brightness)\r\n\r\n    def _hist_equalize(self, image: img_t) -> img_t:\r\n        """Applies histogram equalization to enhance contrast."""\r\n        if len(image.shape) == 2:  # Grayscale\r\n            return cv2.equalizeHist(image)\r\n        else:  # Color image\r\n            ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\r\n            ycrcb[:, :, 0] = cv2.equalizeHist(ycrcb[:, :, 0])\r\n            return cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)\r\n\r\n    def _clahe(self, image: img_t) -> img_t:\r\n        """Applies CLAHE (Contrast Limited Adaptive Histogram Equalization)."""\r\n        clahe = cv2.createCLAHE(clipLimit=self.cfg.CLAHE_clip_limit,\r\n                                tileGridSize=(self.cfg.CLAHE_grid_size, self.cfg.CLAHE_grid_size))\r\n        if len(image.shape) == 2:\r\n            return clahe.apply(image)\r\n        else:\r\n            lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\r\n            lab[:, :, 0] = clahe.apply(lab[:, :, 0])\r\n            return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\r\n\r\n    def _gaussian(self, image: img_t) -> img_t:\r\n        """Applies Gaussian blur for noise reduction."""\r\n        return cv2.GaussianBlur(image,\r\n                                (self.cfg.gaussian_kernel_size,\r\n                                 self.cfg.gaussian_kernel_size),\r\n                                self.cfg.gaussian_sigma)\r\n\r\n    def _median(self, image: img_t) -> img_t:\r\n        """Applies median filtering for salt-and-pepper noise removal."""\r\n        return cv2.medianBlur(image, self.cfg.median_kernel_size)\r\n\r\n    def _bilateral(self, image: img_t) -> img_t:\r\n        """Applies bilateral filtering for noise reduction while preserving edges."""\r\n        return cv2.bilateralFilter(image,\r\n                                   self.cfg.bilateral_d,\r\n                                   self.cfg.bilateral_sigma_color,\r\n                                   self.cfg.bilateral_sigma_space)\r\n\r\n    def _guided_filter(self, image: img_t) -> img_t:\r\n        """Applies guided filtering using OpenCV\'s ximgproc module if available.\r\n        Falls back to bilateral filtering otherwise.\r\n        """\r\n        if hasattr(cv2, "ximgproc") and hasattr(cv2.ximgproc, "guidedFilter"):\r\n            # Apply guided filtering using OpenCV\'s ximgproc module\r\n            return cv2.ximgproc.guidedFilter(image, image, self.cfg.guided_radius, self.cfg.guided_eps)\r\n        else:\r\n            # Fallback method: Apply bilateral filter as an alternative (since guided filtering is edge-preserving)\r\n            print("Warning: Guided filtering is skipped as OpenCV\'s ximgproc module is unavailable. Using Bilateral filter instead.")\r\n            return cv2.bilateralFilter(image,\r\n                                       self.cfg.bilateral_d,\r\n                                       self.cfg.bilateral_sigma_color,\r\n                                       self.cfg.bilateral_sigma_space)\r\n\r\n    def _morph_open(self, image: img_t) -> img_t:\r\n        """Applies morphological opening (erosion followed by dilation) to remove noise."""\r\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,\r\n                                           (self.cfg.morph_open_kernel_size, self.cfg.morph_open_kernel_size))\r\n        return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel, iterations=self.cfg.morph_open_iter)\r\n\r\n    def _morph_close(self, image: img_t) -> img_t:\r\n        """Applies morphological closing (dilation followed by erosion) to close small holes."""\r\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,\r\n                                           (self.cfg.morph_close_kernel_size, self.cfg.morph_close_kernel_size))\r\n        return cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel, iterations=self.cfg.morph_close_iter)\r\n')
    __stickytape_write_module('watershed_detector.py', b'import cv2\r\nimport numpy as np\r\nfrom enum import Enum\r\nfrom typing import List, Sequence, Type \r\nfrom src.detector import *  # Assume this defines a base Detector class\r\nfrom src.type_defs import VizResults, img_t\r\nfrom src.color_defs import Color, compute_hue_std_flip\r\nfrom src.block import Block\r\n# or wherever your PreprocCfg is defined\r\nfrom src.preprocessor import *\r\n\r\n# Example DebugType enum\r\n\r\n\r\n\r\n\r\nclass WatershedDetector(Detector):\r\n    """\r\n    Detect color blocks by:\r\n      1) Using parent-class preprocessing (brightness, blur, etc.)\r\n      2) Watershed-based segmentation for each color\r\n      3) Building Block objects from labeled regions\r\n    """\r\n\r\n    class DebugType(Enum):\r\n        SINGLE_COLOR_MASK = "single_color_mask"\r\n        COMBINED_MASK = "combined_mask"\r\n        WATERSHED_BG = "watershed_bg"\r\n        COMBINED_WATERSHED_BG = "combined_watershed_bg"\r\n        WATERSHED_FG = "watershed_fg"\r\n        COMBINED_WATERSHED_FG = "combined_watershed_fg"\r\n        WATERSHED_DIST_TRANSFORM = "watershed_dist_transform"\r\n        COMBINED_WATERSHED_DIST_TRANSFORM = "combined_watershed_dist_transform"\r\n        WATERSHED_UNKNOWN = "watershed_unknown"\r\n        COMBINED_WATERSHED_UNKNOWN = "combined_watershed_unknown"\r\n        WATERSHED_SEG = "watershed_seg"\r\n        COMBINED_WATERSHED_SEG = "combined_watershed_seg"\r\n    \r\n\r\n    def __init__(\r\n        self,\r\n        detecting_colors: List[Color],\r\n        preproc_cfg: PreprocCfg = PreprocCfg(),\r\n        debug_option: List[DebugType] | bool = []\r\n    ) -> None:\r\n        """\r\n        :param detecting_colors: List of Color objects to detect\r\n        :param preproc_cfg: Preprocessing configuration (inherited from parent)\r\n        :param debug_option: Which debug images to store\r\n        """\r\n        super().__init__(detecting_colors, preproc_cfg, debug_option, self.DebugType)\r\n\r\n        # Additional WatershedDetector-specific parameters\r\n        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\r\n        self.mask_erode_iter = 2\r\n        self.mask_dilate_iter = 2\r\n        self.min_area = 1000   # minimum region area\r\n        self.sure_fg_min_dis_ratio = 0.7\r\n        self.std_threshold_hsv = (3, 50, 50)  # H, S, V thresholds\r\n        self.debug_option : List[WatershedDetector.DebugType]\r\n\r\n    def process_frame(self, frame: img_t) -> List[Block]:\r\n        """\r\n        Main entry point. Uses parent-class preprocessing,\r\n        then applies watershed-based segmentation per color.\r\n        """\r\n        # if selected watershed and it is combined, then in debug_option have to\r\n        # select the single-color version \r\n\r\n        combined_watershed_dbg_imgs = [dbg for dbg in self.debug_option if dbg.value.startswith("combined_watershed")]\r\n        for dbg in combined_watershed_dbg_imgs:\r\n            non_combined = self.DebugType[dbg.value.replace("combined_", "").upper()]\r\n            if non_combined not in self.debug_option:\r\n                print(f"Warning: {dbg.value} is selected but {non_combined.value} is not. ")\r\n\r\n        preprocessed = self._preprocess(frame)\r\n        hsv = cv2.cvtColor(preprocessed, cv2.COLOR_BGR2HSV)\r\n        blocks = self._detect_blocks(hsv, preprocessed)\r\n\r\n        watershed_dbg_imgs_to_combine = [step for step in self.DebugType if \'watershed\' in step.value]\r\n        watershed_dbg_imgs_to_combine = [step for step in watershed_dbg_imgs_to_combine if f"combined_{step.value}" \r\n                                         in [dbg_type.value for dbg_type in self.debug_option]]\r\n\r\n        for step in watershed_dbg_imgs_to_combine:\r\n            self.debug_images[f"combined_{step.value}"] = self._merge_debug_imgs(\r\n                step.value)\r\n\r\n        return blocks\r\n\r\n    def _detect_blocks(self, hsv: img_t, frame_bgr: img_t) -> List[Block]:\r\n        """\r\n        Perform watershed segmentation for each color in self.detecting_colors.\r\n        """\r\n        all_blocks: List[Block] = []\r\n\r\n        # (Optional) create a combined mask for debugging\r\n        combined_mask = np.zeros(hsv.shape[:2], dtype=np.uint8)\r\n\r\n        for color_def in self.detecting_colors:\r\n            # A) Create color-specific mask\r\n            color_mask = self.create_color_mask(hsv, color_def)\r\n\r\n            # Debug: store single color mask\r\n            if self.DebugType.SINGLE_COLOR_MASK in self.debug_option:\r\n                self.debug_images[f"{color_def.name}_mask"] = cv2.cvtColor(\r\n                    color_mask, cv2.COLOR_GRAY2BGR)\r\n\r\n            # Combine to the overall mask (optional, for debugging)\r\n            combined_mask = cv2.bitwise_or(combined_mask, color_mask)\r\n\r\n            # B) Watershed on this color mask\r\n            markers, num_labels, sure_bg, sure_fg, unknown = self._watershed_segment(\r\n                color_mask, color_def, frame_bgr)\r\n\r\n            # Debug images for each color\r\n            # Convert to BGR for consistent display\r\n            unknown_bgr = np.zeros_like(frame_bgr)\r\n            unknown_bgr[unknown == 255] = (128, 128, 128)\r\n\r\n            # C) Build blocks from each labeled region\r\n            h, w = hsv.shape[:2]\r\n            for lbl in range(2, num_labels + 1):\r\n                region_mask = (markers == lbl)\r\n                region_area = np.count_nonzero(region_mask)\r\n                if region_area < self.min_area:\r\n                    continue\r\n\r\n                # Extract region coordinates\r\n                region_pts = np.argwhere(region_mask)\r\n                if len(region_pts) == 0:\r\n                    continue\r\n\r\n                # minAreaRect\r\n                coords_xy = np.fliplr(region_pts).astype(np.float32)  # (x, y)\r\n                coords_xy_list = coords_xy[:, np.newaxis, :]\r\n                rect = cv2.minAreaRect(coords_xy_list)\r\n                (rx, ry), (rw, rh), angle = rect\r\n                # Normalize orientation\r\n                if rw < rh:\r\n                    rw, rh = rh, rw\r\n                    angle += 90\r\n\r\n                # Create label_mask for HSV stats\r\n                label_mask = np.zeros((h, w), dtype=np.uint8)\r\n                label_mask[region_mask] = 255\r\n\r\n                hsv_region = cv2.bitwise_and(hsv, hsv, mask=label_mask)\r\n                # Split channels\r\n                h_ch, s_ch, v_ch = cv2.split(hsv_region)\r\n                h_valid = h_ch[label_mask == 255].astype(np.float32)\r\n                s_valid = s_ch[label_mask == 255].astype(np.float32)\r\n                v_valid = v_ch[label_mask == 255].astype(np.float32)\r\n\r\n                # Compute mean, std\r\n                mean_hsv = cv2.mean(hsv_region, mask=label_mask)[:3]\r\n                std_h = float(compute_hue_std_flip(h_valid))\r\n                std_s = float(np.std(s_valid))\r\n                std_v = float(np.std(v_valid))\r\n\r\n                # Filter by thresholds\r\n                if std_h <= self.std_threshold_hsv[0] and \\\r\n                   std_s <= self.std_threshold_hsv[1] and \\\r\n                   std_v <= self.std_threshold_hsv[2]:\r\n                    # retrieve contour if needed\r\n                    contours, _ = cv2.findContours(\r\n                        label_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n                    if not contours:\r\n                        continue\r\n                    largest_contour = max(contours, key=cv2.contourArea)\r\n\r\n                    block = Block(\r\n                        center=(rx, ry),\r\n                        size=(rw, rh),\r\n                        angle=angle,\r\n                        color=color_def,\r\n                        mean_hsv=(mean_hsv[0], mean_hsv[1], mean_hsv[2]),\r\n                        color_std=(std_h, std_s, std_v),\r\n                        contour=largest_contour\r\n                    )\r\n                    all_blocks.append(block)\r\n\r\n        # Optionally store the combined mask in debug\r\n        if self.DebugType.COMBINED_MASK in self.debug_option:\r\n            combined_bgr = cv2.cvtColor(combined_mask, cv2.COLOR_GRAY2BGR)\r\n            self.debug_images[\'combined_mask\'] = combined_bgr\r\n\r\n        return all_blocks\r\n\r\n    def _watershed_segment(\r\n        self,\r\n        mask: np.ndarray,\r\n        color_def: Color,\r\n        frame_bgr: np.ndarray\r\n    ) -> tuple[np.ndarray, int, np.ndarray, np.ndarray, np.ndarray]:\r\n        """\r\n        Performs distance transform and watershed segmentation.\r\n        Returns (markers, num_labels, sure_bg, sure_fg, unknown).\r\n        Stores debug images if enabled in debug_option.\r\n        """\r\n\r\n        color_name = color_def.name\r\n\r\n        # ------------------- Step 1: Obtain sure background (BG) -------------------\r\n        sure_bg = cv2.dilate(mask, self.kernel, iterations=5)\r\n\r\n        if self.DebugType.WATERSHED_BG in self.debug_option:\r\n            # Convert to BGR format and store\r\n            sure_bg_bgr = cv2.cvtColor(sure_bg, cv2.COLOR_GRAY2BGR)\r\n            sure_bg_bgr[sure_bg == 255] = color_def.bgr\r\n            self.debug_images[f"watershed_bg_{color_name}"] = sure_bg_bgr\r\n\r\n        # ------------------- Step 2: Distance transform to obtain sure foreground (FG) -------------------\r\n        dist_transform = cv2.distanceTransform(\r\n            mask, cv2.DIST_L2, cv2.DIST_MASK_5)\r\n\r\n        # Normalize and apply color map for visualization\r\n        if self.DebugType.WATERSHED_DIST_TRANSFORM in self.debug_option:\r\n            dist_vis = cv2.normalize(dist_transform, dist_transform.copy(),\r\n                                     0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\r\n            dist_color = cv2.applyColorMap(dist_vis, cv2.COLORMAP_JET)\r\n            dist_color[mask == 0] = 0  # Set BG to black\r\n            self.debug_images[f"watershed_dist_transform_{color_name}"] = dist_color\r\n\r\n        max_val = dist_transform.max()\r\n        _, sure_fg = cv2.threshold(dist_transform,\r\n                                   self.sure_fg_min_dis_ratio * max_val,\r\n                                   255, 0)\r\n        sure_fg = sure_fg.astype(np.uint8)\r\n\r\n        if self.DebugType.WATERSHED_FG in self.debug_option:\r\n            # Convert FG to BGR format and store\r\n            sure_fg_bgr = cv2.cvtColor(sure_fg, cv2.COLOR_GRAY2BGR)\r\n            sure_fg_bgr[sure_fg == 255] = color_def.bgr\r\n            self.debug_images[f"watershed_fg_{color_name}"] = sure_fg_bgr\r\n\r\n        # ------------------- Step 3: Compute unknown region -------------------\r\n        unknown = cv2.subtract(sure_bg, sure_fg)\r\n\r\n        if self.DebugType.WATERSHED_UNKNOWN in self.debug_option:\r\n            # Mark unknown region in gray\r\n            unknown_bgr = np.zeros_like(frame_bgr)\r\n            # Gray color for unknown area\r\n            unknown_bgr[unknown == 255] = (128, 128, 128)\r\n            self.debug_images[f"watershed_unknown_{color_name}"] = unknown_bgr\r\n\r\n        # ------------------- Step 4: Label foreground using connected components -------------------\r\n        num_labels, markers = cv2.connectedComponents(sure_fg)\r\n        markers += 1  # Ensure background has label 1\r\n        markers[unknown == 255] = 0  # Mark unknown region with 0\r\n\r\n        # ------------------- Step 5: Apply watershed algorithm -------------------\r\n        cv2.watershed(frame_bgr, markers)\r\n\r\n        # ------------------- Step 6: Visualize segmentation results -------------------\r\n        if self.DebugType.WATERSHED_SEG in self.debug_option:\r\n            seg_vis = np.zeros_like(frame_bgr)\r\n\r\n            rng = np.random.default_rng(42)  # fixed seed for reproducibility\r\n            label_colors = {}\r\n\r\n            # Generate a unique random BGR color for each label\r\n            for lbl in range(2, num_labels + 2):\r\n                color = rng.integers(0, 256, size=3, dtype=np.uint8)\r\n                label_colors[lbl] = color\r\n\r\n            # Overlay color for each labeled region using array mask\r\n            for lbl in range(2, num_labels + 1):\r\n                region_mask = (markers == lbl)  # boolean mask of shape (h, w)\r\n                if not np.any(region_mask):\r\n                    continue\r\n                seg_vis[region_mask] = label_colors[lbl]\r\n\r\n            self.debug_images[f"watershed_seg_{color_name}"] = seg_vis\r\n\r\n        return markers, num_labels, sure_bg, sure_fg, unknown\r\n')
    __stickytape_write_module('meanshift_detector.py', b'import cv2\r\nimport numpy as np\r\nfrom typing import List, TypedDict\r\nfrom src.detector import Detector\r\nfrom src.color_defs import Color, compute_hue_std_flip\r\nfrom src.block import Block\r\n\r\n\r\nclass MeanShiftVizResults(TypedDict, total=False):\r\n    """Typed dictionary structure for debug images."""\r\n    original: np.ndarray\r\n    preprocessed: np.ndarray\r\n    mean_shift_filtered: np.ndarray\r\n    mask_after_threshold: np.ndarray\r\n    final_detection: np.ndarray\r\n\r\n\r\nclass MeanshiftDetector(Detector):\r\n    """\r\n    Detect color blocks by:\r\n      1) Preprocessing (brightness, blur)\r\n      2) Mean Shift filtering to smooth colors\r\n      3) For each color, threshold => morphological cleaning => findContours\r\n      4) Build Block from each contour\r\n    """\r\n\r\n    def __init__(self, detecting_colors: List[Color]):\r\n        # Basic image processing parameters\r\n        self.blur_size = 0\r\n        self.brightness = 0\r\n        self.mask_erode_iter = 0\r\n        self.mask_dilate_iter = 0\r\n        self.min_area = 1000  # Minimum region area\r\n        self.spatial_radius = 30  # For pyrMeanShiftFiltering\r\n        self.color_radius = 50\r\n        self.max_level = 1  # for iterative level in meanShift\r\n        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\r\n\r\n        # Optional HSV std check\r\n        self.std_threshold_hsv = (3, 50, 50)\r\n        self.detecting_colors = detecting_colors\r\n\r\n        # Storage for debug images\r\n        self._debug_images: MeanShiftVizResults = {}\r\n\r\n    def process_frame(self, frame: np.ndarray) -> List[Block]:\r\n        """\r\n        Main entry: preprocess, do mean shift filtering, color threshold, build Blocks\r\n        """\r\n        # Clear old debug images\r\n        self._debug_images = {}\r\n\r\n        # Save original\r\n        self._debug_images[\'original\'] = frame.copy()\r\n\r\n        # 1) Preprocess (e.g. brightness, blur)\r\n        preprocessed = self._preprocess(frame)\r\n        self._debug_images[\'preprocessed\'] = preprocessed.copy()\r\n\r\n        st_time = cv2.getTickCount()\r\n        # 2) Mean Shift filtering\r\n        mean_shift_bgr = cv2.pyrMeanShiftFiltering(\r\n            preprocessed,\r\n            sp=self.spatial_radius,\r\n            sr=self.color_radius,\r\n            maxLevel=self.max_level\r\n        )\r\n        ed_time = cv2.getTickCount()\r\n        print("Mean Shift Time:", (ed_time - st_time) / cv2.getTickFrequency())\r\n        self._debug_images[\'mean_shift_filtered\'] = mean_shift_bgr.copy()\r\n\r\n        # Convert to HSV for color thresholding\r\n        mean_shift_hsv = cv2.cvtColor(mean_shift_bgr, cv2.COLOR_BGR2HSV)\r\n\r\n        # 3) For each color, threshold => morphological => findContours => create Blocks\r\n        blocks: List[Block] = []\r\n        for color_def in self.detecting_colors:\r\n            color_blocks = self._detect_blocks_for_color(\r\n                mean_shift_hsv, mean_shift_bgr, color_def)\r\n            blocks.extend(color_blocks)\r\n\r\n        return blocks\r\n\r\n    def get_debug_images(self) -> MeanShiftVizResults:\r\n        return self._debug_images\r\n\r\n    def _preprocess(self, frame: np.ndarray) -> np.ndarray:\r\n        """Adjust brightness and optionally blur."""\r\n        # Adjust brightness\r\n        frame = cv2.convertScaleAbs(frame, alpha=1, beta=self.brightness)\r\n        # Gaussian blur if needed\r\n        if self.blur_size > 0:\r\n            kernel_size = self.blur_size | 1  # ensure odd\r\n            frame = cv2.GaussianBlur(frame, (kernel_size, kernel_size), 0)\r\n        return frame\r\n\r\n    def _detect_blocks_for_color(self, hsv_img: np.ndarray, bgr_img: np.ndarray, color_def: Color) -> List[Block]:\r\n        """\r\n        For a single color definition, threshold => morphological cleaning => findContours => build Blocks\r\n        """\r\n        blocks: List[Block] = []\r\n\r\n        # A) Combine all HSV ranges for this color\r\n        mask = np.zeros(hsv_img.shape[:2], dtype=np.uint8)\r\n        for (lower, upper) in color_def.hsv_ranges:\r\n            lower_np = np.array(lower, dtype=np.uint8)\r\n            upper_np = np.array(upper, dtype=np.uint8)\r\n            tmp_mask = cv2.inRange(hsv_img, lower_np, upper_np)\r\n            mask = cv2.bitwise_or(mask, tmp_mask)\r\n\r\n        # B) Morphological cleaning\r\n        if self.mask_erode_iter > 0:\r\n            mask = cv2.erode(mask, self.kernel, iterations=self.mask_erode_iter)\r\n        if self.mask_dilate_iter > 0:\r\n            mask = cv2.dilate(mask, self.kernel, iterations=self.mask_dilate_iter)\r\n\r\n        # Optional: store debug for each color\r\n        mask_debug = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\r\n        self._debug_images[f"{color_def.name}_mask_after_threshold"] = mask_debug\r\n\r\n        # C) Find contours in the mask\r\n        contours, _ = cv2.findContours(\r\n            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n        h, w = mask.shape[:2]\r\n        for cnt in contours:\r\n            region_area = cv2.contourArea(cnt)\r\n            if region_area < self.min_area:\r\n                continue\r\n\r\n            # minAreaRect\r\n            rect = cv2.minAreaRect(cnt)\r\n            (rx, ry), (rw, rh), angle = rect\r\n            if rw < rh:\r\n                rw, rh = rh, rw\r\n                angle += 90\r\n\r\n            # Build a mask for this contour to compute stats\r\n            contour_mask = np.zeros((h, w), dtype=np.uint8)\r\n            cv2.drawContours(contour_mask, [cnt], 0, 255, -1)\r\n\r\n            # Extract HSV region\r\n            hsv_region = cv2.bitwise_and(hsv_img, hsv_img, mask=contour_mask)\r\n\r\n            # Mean HSV\r\n            mean_hsv = cv2.mean(hsv_region, mask=contour_mask)[:3]\r\n\r\n            # Std HSV\r\n            h_ch, s_ch, v_ch = cv2.split(hsv_region)\r\n            h_valid = h_ch[contour_mask == 255].astype(np.float32)\r\n            s_valid = s_ch[contour_mask == 255].astype(np.float32)\r\n            v_valid = v_ch[contour_mask == 255].astype(np.float32)\r\n\r\n            std_h = compute_hue_std_flip(h_valid)\r\n            std_s = float(np.std(s_valid))\r\n            std_v = float(np.std(v_valid))\r\n\r\n            # Create Block\r\n            block = Block(\r\n                center=(rx, ry),\r\n                size=(rw, rh),\r\n                angle=angle,\r\n                color=color_def,\r\n                mean_hsv=(mean_hsv[0], mean_hsv[1], mean_hsv[2]),\r\n                color_std=(std_h, std_s, std_v),\r\n                contour=cnt\r\n            )\r\n            blocks.append(block)\r\n\r\n        return blocks\r\n')
    __stickytape_write_module('src/visualizer.py', b'# -------------------- visualizer.py --------------------\r\nimport cv2\r\nimport numpy as np\r\nfrom typing import List, Dict\r\nfrom src.color_defs import *\r\nfrom src.block import Block\r\nfrom src.type_defs import *\r\nfrom src.detector import Detector\r\nfrom src.preprocessor import PreprocType\r\nfrom .debug_controls import DebugControlWidget\r\nfrom PySide6.QtWidgets import QApplication\r\n\r\nNOT_HEADLESS = hasattr(cv2, \'imshow\')\r\n\r\n\r\nclass BlockVisualizer:\r\n    """\r\n    Manages visualization modes with PySide6 debug controls\r\n    """\r\n\r\n    def __init__(self, detector: Detector, show: bool = True):\r\n        self.mode = 0\r\n        self.prev_mode = 0\r\n        self.main_window = "Block Detection"\r\n        self.show = show and NOT_HEADLESS\r\n        self.detector = detector\r\n\r\n        # Initialize GUI components\r\n        self.qt_app = QApplication.instance() or QApplication([])\r\n        self.debug_controls = DebugControlWidget(detector.DebugType)\r\n        self.debug_controls.options_changed.connect(self._update_debug_options)\r\n\r\n        # Initialize states\r\n        self._init_states()\r\n        self.active_windows = set()\r\n        \r\n        self.overlay_dbg_img_on_frame = True\r\n        self.overlay_alpha = 0.5\r\n\r\n    def _init_states(self):\r\n        # Detector states\r\n        detector_states = {\r\n            opt: opt in self.detector.debug_option\r\n            for opt in self.detector.DebugType\r\n        }\r\n\r\n        # Preprocessor states\r\n        preproc_states = {\r\n            step: step in self.detector.preproc.cfg.debug_steps\r\n            for step in PreprocType\r\n        }\r\n\r\n        self.debug_controls.set_states(detector_states, preproc_states)\r\n\r\n    def toggle_mode(self):\r\n        """Switch between mode 0 and mode 1."""\r\n        self.mode = (self.mode + 1) % 2\r\n        if self.mode == 1:\r\n            self.debug_controls.show()\r\n        else:\r\n            self.debug_controls.hide()\r\n\r\n        cv2.destroyAllWindows()\r\n        self.active_windows.clear()\r\n\r\n    def visualize(self, frame: np.ndarray, blocks: List[Block]) -> VizResults:\r\n        """Main visualization processing"""\r\n        self.qt_app.processEvents()\r\n\r\n        debug_images = self.detector.debug_images\r\n\r\n        results: VizResults = {}\r\n\r\n        # Generate final result\r\n        final_result = self.gen_final_result(frame, blocks)\r\n        if self._valid_image(final_result):\r\n            results[\'final detection\'] = final_result\r\n\r\n        # Handle debug mode\r\n        if self.mode == 1:\r\n            if self._valid_image(frame):\r\n                results[\'original\'] = frame.copy()\r\n\r\n            debug_outputs = self.gen_debug_imgs(debug_images, frame)\r\n            results.update(debug_outputs)\r\n\r\n        # Update windows\r\n        if self.show:\r\n            current_windows = set(results.keys())\r\n            if self.mode == 1:\r\n                current_windows.add("Debug Controls")\r\n\r\n            # Close unused windows\r\n            for win in self.active_windows - current_windows:\r\n                cv2.destroyWindow(win)\r\n\r\n            # Update OpenCV windows\r\n            for name, img in results.items():\r\n                if self._valid_image(img):\r\n                    cv2.imshow(name, img)\r\n\r\n            self.active_windows = current_windows\r\n\r\n        return results\r\n\r\n    def _update_debug_options(self):\r\n        """Update detector with current GUI states"""\r\n        detector_states, preproc_states = self.debug_controls.get_states()\r\n\r\n        # Update detector\r\n        self.detector.debug_option = [\r\n            opt for opt, enabled in detector_states.items() if enabled\r\n        ]\r\n\r\n        # Update preprocessor\r\n        self.detector.preproc.cfg.debug_steps = {\r\n            step for step, enabled in preproc_states.items() if enabled\r\n        }\r\n\r\n        print(f"Detector: {self.detector.debug_option}")\r\n        print(f"Preprocessor: {self.detector.preproc.cfg.debug_steps}")\r\n\r\n    def _valid_image(self, img: np.ndarray) -> bool:\r\n        """Validate image dimensions"""\r\n        return (\r\n            isinstance(img, np.ndarray) and\r\n            img.size > 0 and\r\n            img.shape[0] > 0 and\r\n            img.shape[1] > 0\r\n        )\r\n\r\n    def gen_final_result(self, frame: np.ndarray, blocks: List[Block]) -> np.ndarray:\r\n        """Draw bounding boxes and put text for each block."""\r\n        output = frame.copy()\r\n        for block in blocks:\r\n            box = cv2.boxPoints(\r\n                (block.center, block.size, block.angle))  # type: ignore\r\n            box = np.intp(box)\r\n            cv2.drawContours(output, [box], 0,\r\n                             block.color.bgr, 2)  # type: ignore\r\n\r\n            # Text lines with extra info: avgH, avgS, avgV\r\n            lines = [\r\n                f"{block.color.name}: {block.angle:.1f} deg",\r\n                f"stdHSV=({block.color_std[0]:.1f}, {block.color_std[1]:.1f}, {block.color_std[2]:.1f})",\r\n                f"avgHSV=({block.mean_hsv[0]:.1f}, {block.mean_hsv[1]:.1f},{block.mean_hsv[2]:.1f})"\r\n            ]\r\n            x0, y0 = int(block.center[0]), int(block.center[1])\r\n            for i, line in enumerate(lines):\r\n                offset_y = i * 15\r\n                cv2.putText(\r\n                    output,\r\n                    line,\r\n                    (x0, y0 + offset_y),\r\n                    cv2.FONT_HERSHEY_SIMPLEX,\r\n                    0.5,\r\n                    (255, 255, 255),\r\n                    1\r\n                )\r\n\r\n        cv2.putText(\r\n            output,\r\n            "Final Detection",\r\n            (10, 30),\r\n            cv2.FONT_HERSHEY_SIMPLEX,\r\n            1.0,\r\n            (0, 255, 0),\r\n            2\r\n        )\r\n\r\n        return output\r\n\r\n    def gen_debug_imgs(self, debug_images: VizResults, frame : img_t) -> Dict[str, np.ndarray]:\r\n        """\r\n        Only display debug images for which the corresponding checkbox is selected.\r\n        For each debug image, if its name does not start with any of the selected keys\r\n        (and if it has a color suffix, the suffix is separated by an underscore), \r\n        then the corresponding window is destroyed.\r\n        """\r\n        results = {}\r\n        selected_keys = {opt.value for opt in self.detector.debug_option}\r\n        assert isinstance(self.detector.preproc.cfg.debug_steps, set)\r\n        selected_keys.update({step.value for step in self.detector.preproc.cfg.debug_steps})\r\n        for name, img in debug_images.items():\r\n            selected = False\r\n            for key in selected_keys:\r\n                # Check if the debug image name is exactly the key or starts with "key_"\r\n                if name == key or name.startswith(f"{key}_"):\r\n                    selected = True\r\n                    break\r\n    \r\n            if not selected:\r\n                continue\r\n    \r\n            if self._valid_image(img):\r\n                display = img.copy()\r\n                cv2.putText(display, name, (10, 30),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\r\n                results[name] = display\r\n    \r\n        if self.overlay_dbg_img_on_frame:\r\n            for name, img in results.items():\r\n                if name == \'final detection\':\r\n                    continue\r\n                results[name] = cv2.addWeighted(frame, self.overlay_alpha, img, 1 - self.overlay_alpha, 0)\r\n\r\n        return results\r\n')
    __stickytape_write_module('src/debug_controls.py', b'from PySide6.QtWidgets import (\r\n    QWidget, QVBoxLayout, QGroupBox, QCheckBox, QScrollArea\r\n)\r\nfrom PySide6.QtCore import Qt, Signal\r\nfrom typing import Dict, Type, Set, Union\r\nfrom enum import Enum\r\nfrom src.preprocessor import PreprocType\r\n\r\n\r\nclass DebugControlWidget(QWidget):\r\n    options_changed = Signal()\r\n\r\n    def __init__(self, detector_debug_type: Type[Enum], parent=None):\r\n        super().__init__(parent)\r\n        self.detector_states: Dict[Enum, bool] = {}\r\n        self.preproc_states: Dict[PreprocType, bool] = {}\r\n\r\n        self.detector_checkboxes: Dict[Enum, QCheckBox] = {}\r\n        self.preproc_checkboxes: Dict[PreprocType, QCheckBox] = {}\r\n\r\n        self.init_ui(detector_debug_type)\r\n        self.setWindowTitle("Debug Controls")\r\n        self.setMinimumSize(400, 600)\r\n\r\n    def init_ui(self, detector_debug_type: Type[Enum]):\r\n        layout = QVBoxLayout()\r\n        scroll = QScrollArea()\r\n        content = QWidget()\r\n        scroll.setWidget(content)\r\n        scroll.setWidgetResizable(True)\r\n\r\n        main_layout = QVBoxLayout(content)\r\n\r\n        # Detector debug group\r\n        detector_group = QGroupBox("Detector Debug Options")\r\n        detector_layout = QVBoxLayout()\r\n        for opt in detector_debug_type:\r\n            checkbox = QCheckBox(opt.value)\r\n            checkbox.stateChanged.connect(self._handle_detector_change)\r\n            self.detector_states[opt] = False\r\n            self.detector_checkboxes[opt] = checkbox  # \xe8\xae\xb0\xe5\xbd\x95 checkbox\r\n            detector_layout.addWidget(checkbox)\r\n        detector_group.setLayout(detector_layout)\r\n\r\n        # Preprocessor debug group\r\n        preproc_group = QGroupBox("Preprocessor Debug Steps")\r\n        preproc_layout = QVBoxLayout()\r\n        for step in PreprocType:\r\n            checkbox = QCheckBox(step.value)\r\n            checkbox.stateChanged.connect(self._handle_preproc_change)\r\n            self.preproc_states[step] = False\r\n            self.preproc_checkboxes[step] = checkbox  # \xe8\xae\xb0\xe5\xbd\x95 checkbox\r\n            preproc_layout.addWidget(checkbox)\r\n        preproc_group.setLayout(preproc_layout)\r\n\r\n        main_layout.addWidget(detector_group)\r\n        main_layout.addWidget(preproc_group)\r\n        layout.addWidget(scroll)\r\n        self.setLayout(layout)\r\n\r\n    def _handle_detector_change(self, state):\r\n        checkbox = self.sender()\r\n        if checkbox:\r\n            opt = next(\r\n                opt for opt in self.detector_states if opt.value == checkbox.text()\r\n            )\r\n            self.detector_states[opt] = checkbox.isChecked()\r\n            self.options_changed.emit()\r\n\r\n    def _handle_preproc_change(self, state):\r\n        checkbox = self.sender()\r\n        if checkbox:\r\n            step = next(\r\n                step for step in self.preproc_states if step.value == checkbox.text()\r\n            )\r\n            self.preproc_states[step] = checkbox.isChecked()\r\n            self.options_changed.emit()\r\n\r\n    def set_states(self, detector_states: Dict[Enum, bool], preproc_states: Dict[PreprocType, bool]):\r\n        for opt, enabled in detector_states.items():\r\n            self.detector_states[opt] = enabled\r\n            if opt in self.detector_checkboxes:\r\n                self.detector_checkboxes[opt].setChecked(enabled)\r\n\r\n        for step, enabled in preproc_states.items():\r\n            self.preproc_states[step] = enabled\r\n            if step in self.preproc_checkboxes:\r\n                self.preproc_checkboxes[step].setChecked(enabled)\r\n\r\n    def get_states(self):\r\n        return self.detector_states.copy(), self.preproc_states.copy()\r\n')
    __stickytape_write_module('src/utils/serializer.py', b'from src.block import Block\r\nfrom ctypes import Structure, c_int16, c_int32, c_float\r\nimport struct\r\nfrom typing import Tuple, Optional, List\r\nfrom enum import Enum\r\n\r\nclass SerializedColor(Enum):\r\n    YELLOW = 1\r\n    RED = 2\r\n    BLUE = 4\r\n\r\ndef get_serialized_color(color_name: str) -> Optional[SerializedColor]:\r\n    """Maps detected color names to their corresponding SerializedColor enum based on keyword matching."""\r\n    color_name_upper = color_name.upper()\r\n\r\n    for enum_member in SerializedColor:\r\n        if enum_member.name in color_name_upper:\r\n            return enum_member\r\n\r\n    return None  # Return None if no match is found\r\n\r\nclass SerializedBlock(Structure):\r\n    _pack_ = 1  # No padding\r\n    _fields_ = [\r\n        ("center_x", c_int16),\r\n        ("center_y", c_int16),\r\n        ("width", c_int16),\r\n        ("height", c_int16),\r\n        ("angle", c_float),\r\n        ("color", c_int32)\r\n    ]\r\n\r\n    def __str__(self) -> str:\r\n        return f"SerializedBlock(center=({self.center_x}, {self.center_y}), size=({self.width}, {self.height}), angle={self.angle}, color={self.color})"\r\n\r\n    def pack_to_floats(self) -> Tuple[float, float, float, float]:\r\n        """Packs the structure into four floats."""\r\n        raw_bytes = bytes(self)\r\n        assert len(raw_bytes) == 16, f"Expected 16 bytes, got {len(raw_bytes)}"\r\n        f1, f2, f3, f4 = struct.unpack(\'<4f\', raw_bytes)\r\n        return f1, f2, f3, f4\r\n    \r\n    def serialize_to_float(self) -> List[float]:\r\n        """Directly serialize the 6 quantities to floats"""\r\n        return [float(num) for num in [self.center_x, self.center_y, self.width, self.height, self.angle, self.color]]\r\n\r\n    @staticmethod\r\n    def from_block(block: \'Block\') -> \'SerializedBlock\':\r\n        """Creates a SerializedBlock from a Block object."""\r\n        color_enum = get_serialized_color(block.color.name)\r\n        assert color_enum is not None, f"Unknown color: {block.color}"\r\n        return SerializedBlock(\r\n            center_x=int(block.center[0]),  # Ensure int conversion\r\n            center_y=int(block.center[1]),\r\n            width=int(block.size[0]),\r\n            height=int(block.size[1]),\r\n            angle=float(block.angle),  # Ensure float conversion\r\n            color=color_enum.value  # Store enum as int\r\n        )\r\n\r\n    @classmethod\r\n    def from_bytes(cls, raw_bytes: bytes) -> \'SerializedBlock\':\r\n        """Creates a SerializedBlock from raw bytes."""\r\n        assert len(raw_bytes) == 16, f"Expected 16 bytes, got {len(raw_bytes)}"\r\n        return cls.from_buffer_copy(raw_bytes)\r\n\r\n    @classmethod\r\n    def from_floats(cls, f1: float, f2: float, f3: float, f4: float) -> \'SerializedBlock\':\r\n        """Creates a SerializedBlock from four floats."""\r\n        raw_bytes = struct.pack(\'<4f\', f1, f2, f3, f4)\r\n        return cls.from_bytes(raw_bytes)\r\n    \r\n    @classmethod\r\n    def from_raw_floats(cls, fs: List[float]) -> \'SerializedBlock\':\r\n        return SerializedBlock(\r\n            center_x=int(fs[0]),\r\n            center_y=int(fs[1]),\r\n            width=int(fs[2]),\r\n            height=int(fs[3]),\r\n            angle=fs[4],\r\n            color=int(fs[5]),\r\n        )\r\n\r\ndef serialize_to_floats(blocks: List[\'Block\']) -> List[float]:\r\n    """Serializes a list of Blocks into a list of floats."""\r\n    serialized_blocks = [SerializedBlock.from_block(block) for block in blocks]\r\n    return [0, 0] + [f for serialized_block in serialized_blocks for f in serialized_block.serialize_to_float()]\r\n\r\ndef deserialize_from_floats(floats: List[float]) -> List[SerializedBlock]:\r\n    """Deserializes a list of floats into a list of SerializedBlocks."""\r\n    assert len(floats) % 4 == 0, f"Expected a multiple of 4 floats, got {len(floats)}"\r\n    serialized_blocks = [SerializedBlock.from_raw_floats(floats[i:i+6]) for i in range(2, len(floats), 6)]\r\n    return serialized_blocks\r\n')
    __stickytape_write_module('color_defs.py', b'import numpy as np\r\nfrom dataclasses import dataclass, field\r\nfrom typing import List, Tuple\r\nfrom src.type_defs import hsv_t, bgr_t\r\n\r\n@dataclass\r\nclass Color:\r\n    """Stores color name, HSV ranges, and BGR values for drawing."""\r\n    name: str\r\n    hsv_ranges: List[Tuple[hsv_t, hsv_t]]\r\n    bgr: bgr_t\r\n\r\n\r\n# ---------- Color Block Detector ----------\r\n\r\ndef compute_hue_std_flip(h_array: np.ndarray, flip_threshold: float = 90.0) -> float:\r\n    # Ensure float\r\n    h_float = h_array.astype(np.float32)\r\n\r\n    # 1) Direct std\r\n    std1 = np.std(h_float)\r\n\r\n    # 2) Flip\r\n    shifted = h_float.copy()\r\n    mask = (shifted < flip_threshold)\r\n    shifted[mask] += 180.0\r\n    std2 = np.std(shifted)\r\n\r\n    return float(min(std1, std2))\r\n\r\n\r\nRED_R9000P = Color(\r\n    name="RED_R9000P",\r\n    hsv_ranges=[\r\n        ((0, 70, 50), (3, 160, 225)),\r\n        ((165, 70, 50), (180, 160, 225)),\r\n    ],\r\n    bgr=(0, 0, 255)\r\n)\r\n\r\nBLUE_R9000P = Color(\r\n    name="BLUE_R9000P",\r\n    hsv_ranges=[\r\n        ((110, 80, 70), (125, 180, 230)),\r\n    ],\r\n    bgr=(255, 0, 0)\r\n)\r\n\r\nYELLOW_R9000P = Color(\r\n    name="YELLOW_R9000P",\r\n    hsv_ranges=[\r\n        ((17, 60, 140), (32, 125, 255)),\r\n    ],\r\n    bgr=(0, 255, 255)\r\n)\r\n\r\nCOLOR_DEF_R9000P = [RED_R9000P, BLUE_R9000P, YELLOW_R9000P]\r\n\r\nRED_LL = Color(\r\n    name="RED_LL",\r\n    hsv_ranges=[\r\n        ((0, 190, 90), (5, 255, 250)),\r\n        ((160, 190, 90), (180, 255, 250)),\r\n    ],\r\n    bgr=(0, 0, 255)\r\n)\r\n\r\nYELLOW_LL = Color(\r\n    name="YELLOW_LL",\r\n    hsv_ranges=[\r\n        ((15, 130, 160), (35, 255, 250)),\r\n    ],\r\n    bgr=(0, 255, 255)\r\n)\r\n\r\nBLUE_LL = Color(\r\n    name="BLUE_LL",\r\n    hsv_ranges=[\r\n        ((100, 220, 70), (125, 255, 230)),\r\n    ],\r\n    bgr=(255, 0, 0)\r\n)\r\n\r\nCOLOR_DEF_LL = [RED_LL, YELLOW_LL, BLUE_LL]')
    from color_detector import ColorDetector
    from watershed_detector import WatershedDetector
    from meanshift_detector import MeanshiftDetector
    from src.visualizer import BlockVisualizer
    from src.utils.serializer import *
    from color_defs import COLOR_DEF_LL
    from src.utils.serializer import *
    from ctypes import sizeof
    import cv2
    
    MAX_RET_BLK_CNT = 5
    
    def runPipeline(image, llrobot):
        # can change the algorithm with 3 options: contour, watershed, meanshift
        detector = ColorDetector(COLOR_DEF_LL)
        visualizer = BlockVisualizer(detector, show=False)
    
        blocks = detector.process_frame(image)
        image = visualizer.gen_final_result(image, blocks)
    
        # sort the block by area, which can be calculated by contour
        blocks = sorted(blocks, key=lambda x: cv2.contourArea(x.contour), reverse=True)
        
        #select the first MAX_RET_BLK_CNT blocks if exceed
    
        if len(blocks) > MAX_RET_BLK_CNT:
            blocks = blocks[:MAX_RET_BLK_CNT]
    
        serialized_blocks = serialize_to_floats(blocks)
    
        return [], image, serialized_blocks 